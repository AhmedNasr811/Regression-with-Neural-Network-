{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12fe7100",
   "metadata": {},
   "source": [
    "# Regression Using a Neural Network\n",
    "In this Assignment #4, we will implement a feedforward neural network from scratch to predict cement strength using the given dataset.\n",
    "\n",
    "### Names and IDs:\n",
    "#### Abdelrahman Attia Abdelrahman (20206128)\n",
    "#### Shrouk Ashraf Ramdan (20206131)\n",
    "#### Ahmed Nasr Hassan (20206129)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0727c3f6",
   "metadata": {},
   "source": [
    "## First: Let's load and preprocess our data\n",
    "We start by loading the data from the provided excel file, separating the features and target, and normalizing the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e1088bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features shape: (525, 4)\n",
      "Testing features shape: (175, 4)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Loading our dataset\n",
    "file_path = 'concrete_data.xlsx'\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Separating features and target\n",
    "features = data.iloc[:, :-1].values  # Geting the first 4 columns\n",
    "target = data.iloc[:, -1].values    # Geting the last column\n",
    "\n",
    "# Normalizing features\n",
    "scaler = MinMaxScaler()\n",
    "normalized_features = scaler.fit_transform(features)\n",
    "\n",
    "# Spliting into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(normalized_features, target, test_size=0.25, random_state=42)\n",
    "\n",
    "# Checking shapes of traina n test\n",
    "print(f\"Training features shape: {X_train.shape}\")\n",
    "print(f\"Testing features shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87fdff2",
   "metadata": {},
   "source": [
    "## Second: Let's define our Neural Network\n",
    "We will create a custom `NeuralNetwork` class that handles initialization, forward propagation, backward propagation, prediction, and error calculation. This way is better than standalone functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "22d6a5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, num_features, num_neurons_in_hidden_layer, num_predictions, learning_rate):\n",
    "        # Initialize the number of features, neurons in the hidden layer, and output predictions\n",
    "        self.num_features = num_features\n",
    "        self.num_neurons_in_hidden_layer = num_neurons_in_hidden_layer\n",
    "        self.num_predictions = num_predictions\n",
    "        self.learning_rate = learning_rate\n",
    "                \n",
    "        ## We can use either ways for initialization ##\n",
    "        \n",
    "        # Random initialization of weights and biases\n",
    "        self.weights_from_input_to_hidden = np.random.rand(self.num_features, self.num_neurons_in_hidden_layer)\n",
    "        self.bias_for_hidden_layer = np.random.rand(self.num_neurons_in_hidden_layer)\n",
    "        self.weights_from_hidden_to_output = np.random.rand(self.num_neurons_in_hidden_layer, self.num_predictions)\n",
    "        self.bias_for_output_layer = np.random.rand(self.num_predictions)\n",
    "        \n",
    "        # Xavier initialization for weights\n",
    "        #hidden_layer_limit = np.sqrt(6 / (self.num_features + self.num_neurons_in_hidden_layer))\n",
    "        #self.weights_from_input_to_hidden = np.random.uniform(-hidden_layer_limit, hidden_layer_limit, (self.num_features, self.num_neurons_in_hidden_layer))\n",
    "        #self.bias_for_hidden_layer = np.zeros(self.num_neurons_in_hidden_layer)\n",
    "\n",
    "        #output_layer_limit = np.sqrt(6 / (self.num_neurons_in_hidden_layer + self.num_predictions))\n",
    "        #self.weights_from_hidden_to_output = np.random.uniform(-output_layer_limit, output_layer_limit, (self.num_neurons_in_hidden_layer, self.num_predictions))\n",
    "        #self.bias_for_output_layer = np.zeros(self.num_predictions)\n",
    "\n",
    "        \n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    \n",
    "    \n",
    "    def forward_prop(self, inputs):\n",
    "        # Input to hidden layer\n",
    "        self.hidden_layer_input = np.dot(inputs, self.weights_from_input_to_hidden) + self.bias_for_hidden_layer\n",
    "        self.hidden_layer_output = self.sigmoid(self.hidden_layer_input)\n",
    "        \n",
    "        # Hidden to output layer\n",
    "        self.output_layer_input = np.dot(self.hidden_layer_output, self.weights_from_hidden_to_output) + self.bias_for_output_layer\n",
    "        \n",
    "        # Store for use in backward propagation\n",
    "        self.output = self.output_layer_input  # Linear output for regression\n",
    "        \n",
    "        return self.output\n",
    "\n",
    "\n",
    "    \n",
    "    def backward_prop(self, inputs, actual_output, predicted_output):\n",
    "        \n",
    "        # Calculate output error\n",
    "        error = actual_output - predicted_output\n",
    "        \n",
    "        # Compute gradients for output layer\n",
    "        output_layer_gradient = -2 * error  # Derivative of loss with respect to the output\n",
    "        hidden_to_output_weight_update = np.outer(self.hidden_layer_output, output_layer_gradient)\n",
    "        output_bias_update = output_layer_gradient\n",
    "        \n",
    "        # Compute gradients for hidden layer\n",
    "        hidden_layer_error = output_layer_gradient @ self.weights_from_hidden_to_output.T\n",
    "        hidden_layer_gradient = hidden_layer_error * self.sigmoid_derivative(self.hidden_layer_output)\n",
    "        input_to_hidden_weight_update = np.outer(inputs, hidden_layer_gradient)\n",
    "        hidden_bias_update = hidden_layer_gradient\n",
    "\n",
    "        # Update weights and biases\n",
    "        self.weights_from_hidden_to_output -= self.learning_rate * hidden_to_output_weight_update\n",
    "        self.bias_for_output_layer -= self.learning_rate * output_layer_gradient\n",
    "\n",
    "        self.weights_from_input_to_hidden -= self.learning_rate * input_to_hidden_weight_update\n",
    "        self.bias_for_hidden_layer -= self.learning_rate * hidden_layer_gradient\n",
    "\n",
    "    def train(self, training_data, training_labels, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for inputs, actual_output in zip(training_data, training_labels):\n",
    "                predicted_output = self.forward_prop(inputs)\n",
    "                self.backward_prop(inputs, actual_output, predicted_output)\n",
    "                total_loss += (actual_output - predicted_output) ** 2\n",
    "            \n",
    "            if (epoch + 1) % 100 == 0:\n",
    "                print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(training_data)}\")\n",
    "\n",
    "                \n",
    "                \n",
    "    def predict(self, new_data_record):\n",
    "        return self.forward_prop(new_data_record)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def calc_error(self, X_test, y_test):\n",
    "        # Make predictions on the test set\n",
    "        y_pred = np.array([self.predict(x) for x in X_test])\n",
    "\n",
    "        # Calculate mean squared error (MSE)\n",
    "        mse = np.mean((y_test - y_pred.flatten()) ** 2)\n",
    "        return mse\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170576aa",
   "metadata": {},
   "source": [
    "## Third: Let's train our Neural Network\n",
    "Now, time to train the network using the training data for a specified number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e852d73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/1000, Loss: [65.20626636]\n",
      "Epoch 200/1000, Loss: [59.67481446]\n",
      "Epoch 300/1000, Loss: [57.3332437]\n",
      "Epoch 400/1000, Loss: [55.54652724]\n",
      "Epoch 500/1000, Loss: [53.42365703]\n",
      "Epoch 600/1000, Loss: [51.76129296]\n",
      "Epoch 700/1000, Loss: [50.95980553]\n",
      "Epoch 800/1000, Loss: [50.15264839]\n",
      "Epoch 900/1000, Loss: [49.46731329]\n",
      "Epoch 1000/1000, Loss: [48.97239151]\n"
     ]
    }
   ],
   "source": [
    "# Define the neural network architecture\n",
    "input_size = 4  # Number of features\n",
    "hidden_size = 10  # Number of neurons in the hidden layer (can be adjusted and tuned)\n",
    "output_size = 1  # Single output (concrete strength)\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Initialize our neural network\n",
    "neural_network = NeuralNetwork(input_size, hidden_size, output_size, learning_rate)\n",
    "\n",
    "# Train the network\n",
    "epochs = 1000  # Number of training epochs\n",
    "neural_network.train(X_train, y_train, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b78a5a",
   "metadata": {},
   "source": [
    "## Finally: Let's evaluate and predict\n",
    "We're going to Use the trained network to make predictions and evaluate its performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b94ea0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on Test Set: 49.064507480434074\n"
     ]
    }
   ],
   "source": [
    "# Calculate mean squared error (MSE)\n",
    "mse = neural_network.calc_error(X_test, y_test)\n",
    "print(f\"Mean Squared Error on Test Set: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd691833",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb724d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
